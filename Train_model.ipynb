{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/johann/anaconda3/envs/pytorch/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import random\n",
    "\n",
    "class ImageClassificationDataset(Dataset):\n",
    "    def __init__(self, positive_dir, negative_dir, transform=None):\n",
    "        self.positive_dir = positive_dir\n",
    "        self.negative_dir = negative_dir\n",
    "        self.transform = transform\n",
    "        self.positive_files = [(os.path.join(positive_dir, f), 1) for f in os.listdir(positive_dir) if f.endswith('.png')]\n",
    "        self.negative_files = [(os.path.join(negative_dir, f), 0) for f in os.listdir(negative_dir) if f.endswith('.png')]\n",
    "        self.all_files = self.positive_files + self.negative_files\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.all_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        file_path, label = self.all_files[idx]\n",
    "        image = Image.open(file_path).convert(\"RGB\")\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image, label, os.path.basename(file_path)\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "positive_dir = \"data/train_images/Train/Positive\"\n",
    "negative_dir = \"data/train_images/Train/Negative\"\n",
    "\n",
    "# use an equal number of positive and negative samples for training\n",
    "positive_files = [f for f in os.listdir(positive_dir) if f.endswith('.png')]\n",
    "negative_files = [f for f in os.listdir(negative_dir) if f.endswith('.png')]\n",
    "n = min(len(positive_files), len(negative_files))\n",
    "positive_files = positive_files[:n]\n",
    "negative_files = negative_files[:n]\n",
    "\n",
    "# dataset = ImageClassificationDataset(positive_dir, negative_dir, transform)\n",
    "# dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "# suffle split train and test data 85/15\n",
    "random.shuffle(positive_files)\n",
    "random.shuffle(negative_files)\n",
    "split = int(0.85 * n)\n",
    "train_positive_files = positive_files[:split]\n",
    "train_negative_files = negative_files[:split]\n",
    "test_positive_files = positive_files[split:]\n",
    "\n",
    "train_dataset = ImageClassificationDataset(positive_dir, negative_dir, transform)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=256, shuffle=True)\n",
    "\n",
    "test_dataset = ImageClassificationDataset(positive_dir, negative_dir, transform)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=256, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "        self.fc1 = nn.Linear(64 * 32 * 32, 128)\n",
    "        self.fc2 = nn.Linear(128, 2)  # Output: 2 classes (positive and negative)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 64 * 32 * 32)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "model = SimpleCNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/15], Loss: 0.8146\n",
      "Epoch [2/15], Loss: 0.6625\n",
      "Epoch [3/15], Loss: 0.6116\n",
      "Epoch [4/15], Loss: 0.5371\n",
      "Epoch [5/15], Loss: 0.4575\n",
      "Epoch [6/15], Loss: 0.4141\n",
      "Epoch [7/15], Loss: 0.3859\n",
      "Epoch [8/15], Loss: 0.3577\n",
      "Epoch [9/15], Loss: 0.3279\n",
      "Epoch [10/15], Loss: 0.3067\n",
      "Epoch [11/15], Loss: 0.2990\n",
      "Epoch [12/15], Loss: 0.2824\n",
      "Epoch [13/15], Loss: 0.2632\n",
      "Epoch [14/15], Loss: 0.2542\n",
      "Epoch [15/15], Loss: 0.2539\n",
      "Training complete.\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 15\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for images, labels, _ in train_dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_dataloader):.4f}\")\n",
    "\n",
    "print(\"Training complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved at data/train_images/Train/models/model.pth\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SimpleCNN(\n",
       "  (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (fc1): Linear(in_features=65536, out_features=128, bias=True)\n",
       "  (fc2): Linear(in_features=128, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#store the model in \"data/train_images/Train/models/\"\n",
    "model_dir = \"data/train_images/Train/models\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "model_path = os.path.join(model_dir, \"model.pth\")\n",
    "torch.save(model.state_dict(), model_path)\n",
    "print(f\"Model saved at {model_path}\")\n",
    "\n",
    "# Load the model\n",
    "model = SimpleCNN()\n",
    "model.load_state_dict(torch.load(model_path))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File: mask_20241203-163804-498_117_146_30.png, Prediction: 1, Label: 1\n",
      "File: mask_20241126-162109-143_x_-2_y_-23_rotation_322.png, Prediction: 1, Label: 0\n",
      "Image not found\n",
      "File: mask_20241126-144530-138_x_-13_y_-57_rotation_58.png, Prediction: 0, Label: 1\n",
      "File: mask_20241204-092157-059_x_-45_y_-58_rotation_190.png, Prediction: 0, Label: 0\n",
      "Image not found\n",
      "File: mask_20241203-084310-762_x_-53_y_-7_rotation_89.png, Prediction: 0, Label: 0\n",
      "Image not found\n",
      "File: mask_20241202-164658-012_x_-36_y_-41_rotation_254.png, Prediction: 1, Label: 0\n",
      "Image not found\n",
      "File: mask_20241204-092650-340_x_-19_y_-23_rotation_197.png, Prediction: 0, Label: 0\n",
      "Image not found\n",
      "File: mask_20241202-114443-932_103_117_0.png, Prediction: 1, Label: 1\n",
      "File: mask_20241202-145830-150_x_-33_y_-73_rotation_130.png, Prediction: 1, Label: 1\n",
      "File: mask_20241203-164400-558_x_-23_y_-85_rotation_344.png, Prediction: 1, Label: 1\n",
      "File: mask_20241202-104956-446_x_-30_y_-16_rotation_95.png, Prediction: 0, Label: 0\n",
      "Image not found\n",
      "File: mask_20241203-085230-680_x_-12_y_-55_rotation_123.png, Prediction: 0, Label: 0\n",
      "Image not found\n",
      "File: mask_20241204-092403-849_x_-3_y_-82_rotation_75.png, Prediction: 1, Label: 0\n",
      "Image not found\n",
      "File: mask_20241202-164917-823_100_101_0.png, Prediction: 1, Label: 1\n",
      "File: mask_20241202-164707-225_153_114_35.png, Prediction: 1, Label: 1\n",
      "File: mask_20241202-164022-618_x_-49_y_-45_rotation_237.png, Prediction: 1, Label: 1\n",
      "File: mask_20241202-164713-163_42_90_105.png, Prediction: 1, Label: 1\n",
      "File: mask_20241203-085218-615_x_-13_y_-68_rotation_158.png, Prediction: 0, Label: 0\n",
      "Image not found\n",
      "File: mask_20241202-165200-725_76_68_255.png, Prediction: 1, Label: 1\n",
      "File: mask_20241203-170834-911_x_-104_y_-186_rotation_55.png, Prediction: 1, Label: 0\n",
      "Image not found\n",
      "Accuracy: 0.88\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opening in existing browser session.\n",
      "Opening in existing browser session.\n",
      "Opening in existing browser session.\n",
      "Opening in existing browser session.\n",
      "Opening in existing browser session.\n",
      "Opening in existing browser session.\n",
      "Opening in existing browser session.\n",
      "Opening in existing browser session.\n",
      "Opening in existing browser session.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[180670:180670:0106/181650.339532:ERROR:object_proxy.cc(576)] Failed to call method: org.freedesktop.ScreenSaver.GetActive: object_path= /org/freedesktop/ScreenSaver: org.freedesktop.DBus.Error.NotSupported: This method is not part of the idle inhibition specification: https://specifications.freedesktop.org/idle-inhibit-spec/latest/\n",
      "Created TensorFlow Lite XNNPACK delegate for CPU.\n",
      "Attempting to use a delegate that only supports static-sized tensors with a graph that has dynamic-sized tensors (tensor#141 is a dynamic-sized tensor).\n"
     ]
    }
   ],
   "source": [
    "# Function to evaluate the model and output file names\n",
    "def evaluate_model(model, test_dataloader):\n",
    "    model.eval()\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "    all_filenames = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels, filenames in test_dataloader:\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            all_predictions.extend(predicted.numpy())\n",
    "            all_labels.extend(labels.numpy())\n",
    "            all_filenames.extend(filenames)\n",
    "\n",
    "    return all_predictions, all_labels, all_filenames\n",
    "\n",
    "# # Evaluate the model\n",
    "# test_dataset = ImageClassificationDataset(positive_dir, negative_dir, transform)\n",
    "# test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "predictions, labels, filenames = evaluate_model(model, test_dataloader)\n",
    "\n",
    "positive_dir = \"data/train_images/Train/Positive\"\n",
    "# Print the results\n",
    "idx = 0\n",
    "for filename, prediction, label in zip(filenames, predictions, labels):\n",
    "    print(f\"File: {filename}, Prediction: {prediction}, Label: {label}\")\n",
    "    # print the original image\n",
    "    try:\n",
    "        image = Image.open(os.path.join(positive_dir, filename))\n",
    "        image.show()\n",
    "    except:\n",
    "        print(\"Image not found\")\n",
    "    #image = Image.open(os.path.join(positive_dir, filename))\n",
    "    # image.show()\n",
    "    idx += 1\n",
    "    if idx == 20:\n",
    "        break\n",
    "\n",
    "# print accuracy\n",
    "correct = sum([1 if p == l else 0 for p, l in zip(predictions, labels)])\n",
    "accuracy = correct / len(labels)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
